Single-Cell Differential Abundance (DA) Analysis Benchmark

ğŸ“Š Overview
This repository contains the comprehensive benchmark implementation for evaluating 16 differential abundance (DA) analysis methods on single-cell RNA sequencing data, as described in our publication. The benchmark includes evaluation code, workflows, simulated datasets, real datasets, and metrics computation for thorough method assessment.


ğŸ¯ Evaluated Methods
We systematically evaluated 16 state-of-the-art DA methods for single-cell data analysis. The complete list of methods and their implementations are detailed in our publication.


ğŸ” Method Selection Criteria
Methods were selected based on the following rigorous criteria:


Criteria
âœ… Method must be currently functional and maintained	
âœ… Specifically designed for single-cell data analysis	
âœ… Primary intent must be differential abundance analysis	

ğŸš« Exclusion Criteria
Several methods were excluded from our benchmark:

âŒ scellpam: Primarily a clustering method, not designed for DA analysis

âŒ TascCODA: Requires construction of cell-type hierarchy trees, dependent on well-defined cell-type relationships

âŒ Other exclusions: Methods that were deprecated, not maintained, or primarily designed for other analytical purposes

ğŸ“ Repository Structure
benchmark/
â”œâ”€â”€ ğŸ“‚ pipeline/
â”‚   â”œâ”€â”€ evaluation_pipeline/      # Main evaluation workflows
â”‚   
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ simulated_datasets/       # Synthetic data for controlled evaluation
â”‚   â””â”€â”€ real_datasets/           # Biological datasets for validation
â”œâ”€â”€ ğŸ“‚ results/                  # Pre-computed benchmark results
â””â”€â”€ ğŸ“‚ config/                   # Configuration files

âš ï¸ Important Notes & Limitations
ğŸ“ Disclaimer: This benchmark provides an initial comprehensive evaluation of DA methods, but several limitations should be considered:

ğŸ”¬ Current Limitations
ğŸ“ Evaluation Depth: The current analysis provides a broad overview rather than deep, method-specific optimization

ğŸ”§ Parameter Sensitivity: Limited exploration of parameter spaces for each method

ğŸŒ Dataset Scope: Evaluation on a curated but not exhaustive set of biological scenarios

â±ï¸ Computational Resources: Performance metrics may vary with different computational environments



ğŸ¯ Primary Goals
This benchmark aims to:

Help researchers select appropriate DA methods for their specific biological questions

Provide transparent and reproducible evaluation workflows

Highlight method strengths and weaknesses across different scenarios

Serve as a foundation for future, more specialized benchmarks


â­ If you find this benchmark useful, please consider starring this repository!



